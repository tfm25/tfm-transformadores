{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c470fe71-d608-41bd-ae23-a5335426d5be",
   "metadata": {},
   "source": [
    "# **Leer data transformadores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c12978-8472-4e5c-84c7-230e2483a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha y hora</th>\n",
       "      <th>SE_Valparaiso.Trf_Valparaiso_T1 Potencia activa media (kW)</th>\n",
       "      <th>SE_Valparaiso.Trf_Valparaiso_T1 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_Placeres.Trf_Placeres_T1 Potencia activa media (kW)</th>\n",
       "      <th>SE_Placeres.Trf_Placeres_T1 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_Placeres.Trf_Placeres_T2 Potencia activa media (kW)</th>\n",
       "      <th>SE_Placeres.Trf_Placeres_T2 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_Placilla.Trf_Placilla_T2 Potencia activa media (kW)</th>\n",
       "      <th>SE_Placilla.Trf_Placilla_T2 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)</th>\n",
       "      <th>...</th>\n",
       "      <th>SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_Calera.Trf_Calera_T2 Potencia activa media (kW)</th>\n",
       "      <th>SE_Calera.Trf_Calera_T2 Potencia reactiva media (kVAr)</th>\n",
       "      <th>estacion</th>\n",
       "      <th>estacion_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01 00:30:00</td>\n",
       "      <td>6657.061100</td>\n",
       "      <td>392.181486</td>\n",
       "      <td>9087.688446</td>\n",
       "      <td>-89.524969</td>\n",
       "      <td>9393.958092</td>\n",
       "      <td>1439.777136</td>\n",
       "      <td>8747.844727</td>\n",
       "      <td>-132.386734</td>\n",
       "      <td>4363.275051</td>\n",
       "      <td>...</td>\n",
       "      <td>8986.115234</td>\n",
       "      <td>1787.867920</td>\n",
       "      <td>14563.25817</td>\n",
       "      <td>1948.054075</td>\n",
       "      <td>8607.710838</td>\n",
       "      <td>1346.077800</td>\n",
       "      <td>7858.459949</td>\n",
       "      <td>467.431158</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01 00:45:00</td>\n",
       "      <td>6600.084782</td>\n",
       "      <td>370.762348</td>\n",
       "      <td>9007.225990</td>\n",
       "      <td>-48.079155</td>\n",
       "      <td>9353.917122</td>\n",
       "      <td>1430.533409</td>\n",
       "      <td>8626.563477</td>\n",
       "      <td>-140.524490</td>\n",
       "      <td>4372.547626</td>\n",
       "      <td>...</td>\n",
       "      <td>8946.944336</td>\n",
       "      <td>1806.897827</td>\n",
       "      <td>14303.06339</td>\n",
       "      <td>2033.608913</td>\n",
       "      <td>8421.904564</td>\n",
       "      <td>1292.887449</td>\n",
       "      <td>7973.084927</td>\n",
       "      <td>486.736864</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 01:00:00</td>\n",
       "      <td>6579.427719</td>\n",
       "      <td>385.641813</td>\n",
       "      <td>8908.111572</td>\n",
       "      <td>-71.154483</td>\n",
       "      <td>9084.967613</td>\n",
       "      <td>1380.251646</td>\n",
       "      <td>8546.049805</td>\n",
       "      <td>-164.159393</td>\n",
       "      <td>4297.423363</td>\n",
       "      <td>...</td>\n",
       "      <td>8749.440430</td>\n",
       "      <td>1736.990967</td>\n",
       "      <td>14161.10229</td>\n",
       "      <td>2011.147261</td>\n",
       "      <td>8396.206856</td>\n",
       "      <td>1296.282291</td>\n",
       "      <td>7781.050682</td>\n",
       "      <td>708.620131</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01 01:15:00</td>\n",
       "      <td>6523.938179</td>\n",
       "      <td>389.757663</td>\n",
       "      <td>8725.055695</td>\n",
       "      <td>-123.746052</td>\n",
       "      <td>8897.534370</td>\n",
       "      <td>1328.508615</td>\n",
       "      <td>8458.301758</td>\n",
       "      <td>-164.846237</td>\n",
       "      <td>4189.702034</td>\n",
       "      <td>...</td>\n",
       "      <td>8650.058594</td>\n",
       "      <td>1721.460083</td>\n",
       "      <td>14255.73540</td>\n",
       "      <td>2104.893208</td>\n",
       "      <td>8420.323372</td>\n",
       "      <td>1306.474209</td>\n",
       "      <td>7601.181507</td>\n",
       "      <td>1331.551671</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01 01:30:00</td>\n",
       "      <td>6389.933586</td>\n",
       "      <td>364.311010</td>\n",
       "      <td>8571.299553</td>\n",
       "      <td>-159.668446</td>\n",
       "      <td>8714.198112</td>\n",
       "      <td>1283.114433</td>\n",
       "      <td>8302.192383</td>\n",
       "      <td>-205.847855</td>\n",
       "      <td>4165.856361</td>\n",
       "      <td>...</td>\n",
       "      <td>8527.763672</td>\n",
       "      <td>1702.901367</td>\n",
       "      <td>13911.45802</td>\n",
       "      <td>1989.736915</td>\n",
       "      <td>8410.838127</td>\n",
       "      <td>1365.293384</td>\n",
       "      <td>7718.300819</td>\n",
       "      <td>1400.925994</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Fecha y hora  \\\n",
       "0  2021-01-01 00:30:00   \n",
       "1  2021-01-01 00:45:00   \n",
       "2  2021-01-01 01:00:00   \n",
       "3  2021-01-01 01:15:00   \n",
       "4  2021-01-01 01:30:00   \n",
       "\n",
       "   SE_Valparaiso.Trf_Valparaiso_T1 Potencia activa media (kW)  \\\n",
       "0                                        6657.061100            \n",
       "1                                        6600.084782            \n",
       "2                                        6579.427719            \n",
       "3                                        6523.938179            \n",
       "4                                        6389.933586            \n",
       "\n",
       "   SE_Valparaiso.Trf_Valparaiso_T1 Potencia reactiva media (kVAr)  \\\n",
       "0                                         392.181486                \n",
       "1                                         370.762348                \n",
       "2                                         385.641813                \n",
       "3                                         389.757663                \n",
       "4                                         364.311010                \n",
       "\n",
       "   SE_Placeres.Trf_Placeres_T1 Potencia activa media (kW)  \\\n",
       "0                                        9087.688446        \n",
       "1                                        9007.225990        \n",
       "2                                        8908.111572        \n",
       "3                                        8725.055695        \n",
       "4                                        8571.299553        \n",
       "\n",
       "   SE_Placeres.Trf_Placeres_T1 Potencia reactiva media (kVAr)  \\\n",
       "0                                         -89.524969            \n",
       "1                                         -48.079155            \n",
       "2                                         -71.154483            \n",
       "3                                        -123.746052            \n",
       "4                                        -159.668446            \n",
       "\n",
       "   SE_Placeres.Trf_Placeres_T2 Potencia activa media (kW)  \\\n",
       "0                                        9393.958092        \n",
       "1                                        9353.917122        \n",
       "2                                        9084.967613        \n",
       "3                                        8897.534370        \n",
       "4                                        8714.198112        \n",
       "\n",
       "   SE_Placeres.Trf_Placeres_T2 Potencia reactiva media (kVAr)  \\\n",
       "0                                        1439.777136            \n",
       "1                                        1430.533409            \n",
       "2                                        1380.251646            \n",
       "3                                        1328.508615            \n",
       "4                                        1283.114433            \n",
       "\n",
       "   SE_Placilla.Trf_Placilla_T2 Potencia activa media (kW)  \\\n",
       "0                                        8747.844727        \n",
       "1                                        8626.563477        \n",
       "2                                        8546.049805        \n",
       "3                                        8458.301758        \n",
       "4                                        8302.192383        \n",
       "\n",
       "   SE_Placilla.Trf_Placilla_T2 Potencia reactiva media (kVAr)  \\\n",
       "0                                        -132.386734            \n",
       "1                                        -140.524490            \n",
       "2                                        -164.159393            \n",
       "3                                        -164.846237            \n",
       "4                                        -205.847855            \n",
       "\n",
       "   SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)  ...  \\\n",
       "0                                        4363.275051     ...   \n",
       "1                                        4372.547626     ...   \n",
       "2                                        4297.423363     ...   \n",
       "3                                        4189.702034     ...   \n",
       "4                                        4165.856361     ...   \n",
       "\n",
       "   SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)  \\\n",
       "0                                        8986.115234            \n",
       "1                                        8946.944336            \n",
       "2                                        8749.440430            \n",
       "3                                        8650.058594            \n",
       "4                                        8527.763672            \n",
       "\n",
       "   SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)  \\\n",
       "0                                        1787.867920                \n",
       "1                                        1806.897827                \n",
       "2                                        1736.990967                \n",
       "3                                        1721.460083                \n",
       "4                                        1702.901367                \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)  \\\n",
       "0                                        14563.25817            \n",
       "1                                        14303.06339            \n",
       "2                                        14161.10229            \n",
       "3                                        14255.73540            \n",
       "4                                        13911.45802            \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)  \\\n",
       "0                                        1948.054075                \n",
       "1                                        2033.608913                \n",
       "2                                        2011.147261                \n",
       "3                                        2104.893208                \n",
       "4                                        1989.736915                \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)  \\\n",
       "0                                        8607.710838            \n",
       "1                                        8421.904564            \n",
       "2                                        8396.206856            \n",
       "3                                        8420.323372            \n",
       "4                                        8410.838127            \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)  \\\n",
       "0                                        1346.077800                \n",
       "1                                        1292.887449                \n",
       "2                                        1296.282291                \n",
       "3                                        1306.474209                \n",
       "4                                        1365.293384                \n",
       "\n",
       "   SE_Calera.Trf_Calera_T2 Potencia activa media (kW)  \\\n",
       "0                                        7858.459949    \n",
       "1                                        7973.084927    \n",
       "2                                        7781.050682    \n",
       "3                                        7601.181507    \n",
       "4                                        7718.300819    \n",
       "\n",
       "   SE_Calera.Trf_Calera_T2 Potencia reactiva media (kVAr)  estacion  \\\n",
       "0                                         467.431158         verano   \n",
       "1                                         486.736864         verano   \n",
       "2                                         708.620131         verano   \n",
       "3                                        1331.551671         verano   \n",
       "4                                        1400.925994         verano   \n",
       "\n",
       "   estacion_2  \n",
       "0      verano  \n",
       "1      verano  \n",
       "2      verano  \n",
       "3      verano  \n",
       "4      verano  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lee el archivo CSV\n",
    "df_transformadores = pd.read_csv('df_rellenado_con_nans.csv')\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "df_transformadores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a680f4b-4db0-448d-ab8b-776545dff7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['verano', 'invierno'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformadores['estacion_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf1d38b1-4fec-4afa-aaff-04a0a90af4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = [\n",
    "    'Fecha y hora',\n",
    "    \"SE_San_Rafael.Trf_San_Rafael_T1 Potencia activa media (kW)\",\n",
    "    \"SE_San_Rafael.Trf_San_Rafael_T1 Potencia reactiva media (kVAr)\",\n",
    "    \"SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)\",\n",
    "    \"SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)\",\n",
    "    \"SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)\",\n",
    "    \"SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)\",\n",
    "    \"SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)\",\n",
    "    \"SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)\",\n",
    "    \"SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)\",\n",
    "    \"SE_Concon.Trf_Concon_T2_2 Potencia reactiva media (kVAr)\",\n",
    "    'estacion',\n",
    "    'estacion_2'\n",
    "]\n",
    "\n",
    "df_transformadores = df_transformadores[columnas]\n",
    "\n",
    "df_transformadores['Fecha y hora'] = pd.to_datetime(df_transformadores['Fecha y hora'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a324baea-0740-4e4a-bd69-9f3aa6ee1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "transformador_columnas_p = [\n",
    " \"SE_San_Rafael.Trf_San_Rafael_T1 Potencia activa media (kW)\",\n",
    " \"SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)\",\n",
    " \"SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)\",\n",
    " \"SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)\", \n",
    " \"SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)\"\n",
    "]\n",
    "    \n",
    "\n",
    "transformador_columnas_q = [\n",
    " \"SE_San_Rafael.Trf_San_Rafael_T1 Potencia reactiva media (kVAr)\", \n",
    " \"SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)\", \n",
    " \"SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)\",\n",
    " \"SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)\",\n",
    "\"SE_Concon.Trf_Concon_T2_2 Potencia reactiva media (kVAr)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03667da6-c4a3-4bba-88bd-3d92daf451df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir ambas listas de columnas\n",
    "columnas_a_convertir = transformador_columnas_p + transformador_columnas_q\n",
    "\n",
    "# Aplicar la conversión: dividir por 1000\n",
    "df_transformadores[columnas_a_convertir] = df_transformadores[columnas_a_convertir] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3080edc-94e6-4d93-a818-a6ccef53ad39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha y hora</th>\n",
       "      <th>SE_San_Rafael.Trf_San_Rafael_T1 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Rafael.Trf_San_Rafael_T1 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)</th>\n",
       "      <th>SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)</th>\n",
       "      <th>SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)</th>\n",
       "      <th>SE_Concon.Trf_Concon_T2_2 Potencia reactiva media (kVAr)</th>\n",
       "      <th>estacion</th>\n",
       "      <th>estacion_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01 00:30:00</td>\n",
       "      <td>12.940086</td>\n",
       "      <td>3.649591</td>\n",
       "      <td>8.986115</td>\n",
       "      <td>1.787868</td>\n",
       "      <td>14.563258</td>\n",
       "      <td>1.948054</td>\n",
       "      <td>8.607711</td>\n",
       "      <td>1.346078</td>\n",
       "      <td>4.363275</td>\n",
       "      <td>0.705140</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01 00:45:00</td>\n",
       "      <td>12.640568</td>\n",
       "      <td>3.558793</td>\n",
       "      <td>8.946944</td>\n",
       "      <td>1.806898</td>\n",
       "      <td>14.303063</td>\n",
       "      <td>2.033609</td>\n",
       "      <td>8.421905</td>\n",
       "      <td>1.292887</td>\n",
       "      <td>4.372548</td>\n",
       "      <td>0.669971</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01 01:00:00</td>\n",
       "      <td>12.424030</td>\n",
       "      <td>3.533698</td>\n",
       "      <td>8.749440</td>\n",
       "      <td>1.736991</td>\n",
       "      <td>14.161102</td>\n",
       "      <td>2.011147</td>\n",
       "      <td>8.396207</td>\n",
       "      <td>1.296282</td>\n",
       "      <td>4.297423</td>\n",
       "      <td>0.676845</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01 01:15:00</td>\n",
       "      <td>12.373395</td>\n",
       "      <td>3.551495</td>\n",
       "      <td>8.650059</td>\n",
       "      <td>1.721460</td>\n",
       "      <td>14.255735</td>\n",
       "      <td>2.104893</td>\n",
       "      <td>8.420323</td>\n",
       "      <td>1.306474</td>\n",
       "      <td>4.189702</td>\n",
       "      <td>0.644257</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01 01:30:00</td>\n",
       "      <td>12.171638</td>\n",
       "      <td>3.438511</td>\n",
       "      <td>8.527764</td>\n",
       "      <td>1.702901</td>\n",
       "      <td>13.911458</td>\n",
       "      <td>1.989737</td>\n",
       "      <td>8.410838</td>\n",
       "      <td>1.365293</td>\n",
       "      <td>4.165856</td>\n",
       "      <td>0.633751</td>\n",
       "      <td>verano</td>\n",
       "      <td>verano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Fecha y hora  \\\n",
       "0 2021-01-01 00:30:00   \n",
       "1 2021-01-01 00:45:00   \n",
       "2 2021-01-01 01:00:00   \n",
       "3 2021-01-01 01:15:00   \n",
       "4 2021-01-01 01:30:00   \n",
       "\n",
       "   SE_San_Rafael.Trf_San_Rafael_T1 Potencia activa media (kW)  \\\n",
       "0                                          12.940086            \n",
       "1                                          12.640568            \n",
       "2                                          12.424030            \n",
       "3                                          12.373395            \n",
       "4                                          12.171638            \n",
       "\n",
       "   SE_San_Rafael.Trf_San_Rafael_T1 Potencia reactiva media (kVAr)  \\\n",
       "0                                           3.649591                \n",
       "1                                           3.558793                \n",
       "2                                           3.533698                \n",
       "3                                           3.551495                \n",
       "4                                           3.438511                \n",
       "\n",
       "   SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)  \\\n",
       "0                                           8.986115            \n",
       "1                                           8.946944            \n",
       "2                                           8.749440            \n",
       "3                                           8.650059            \n",
       "4                                           8.527764            \n",
       "\n",
       "   SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)  \\\n",
       "0                                           1.787868                \n",
       "1                                           1.806898                \n",
       "2                                           1.736991                \n",
       "3                                           1.721460                \n",
       "4                                           1.702901                \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)  \\\n",
       "0                                          14.563258            \n",
       "1                                          14.303063            \n",
       "2                                          14.161102            \n",
       "3                                          14.255735            \n",
       "4                                          13.911458            \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)  \\\n",
       "0                                           1.948054                \n",
       "1                                           2.033609                \n",
       "2                                           2.011147                \n",
       "3                                           2.104893                \n",
       "4                                           1.989737                \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)  \\\n",
       "0                                           8.607711            \n",
       "1                                           8.421905            \n",
       "2                                           8.396207            \n",
       "3                                           8.420323            \n",
       "4                                           8.410838            \n",
       "\n",
       "   SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)  \\\n",
       "0                                           1.346078                \n",
       "1                                           1.292887                \n",
       "2                                           1.296282                \n",
       "3                                           1.306474                \n",
       "4                                           1.365293                \n",
       "\n",
       "   SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)  \\\n",
       "0                                           4.363275      \n",
       "1                                           4.372548      \n",
       "2                                           4.297423      \n",
       "3                                           4.189702      \n",
       "4                                           4.165856      \n",
       "\n",
       "   SE_Concon.Trf_Concon_T2_2 Potencia reactiva media (kVAr) estacion  \\\n",
       "0                                           0.705140          verano   \n",
       "1                                           0.669971          verano   \n",
       "2                                           0.676845          verano   \n",
       "3                                           0.644257          verano   \n",
       "4                                           0.633751          verano   \n",
       "\n",
       "  estacion_2  \n",
       "0     verano  \n",
       "1     verano  \n",
       "2     verano  \n",
       "3     verano  \n",
       "4     verano  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transformadores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db5c3ffc-4b0c-4e0a-b9e6-dc0b0619c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha mínima: 2021-01-01 00:30:00\n",
      "Fecha máxima: 2025-04-21 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df_transformadores['Fecha y hora'] = pd.to_datetime(df_transformadores['Fecha y hora'])\n",
    "\n",
    "# Mostrar la fecha mínima y máxima\n",
    "fecha_min = df_transformadores['Fecha y hora'].min()\n",
    "fecha_max = df_transformadores['Fecha y hora'].max()\n",
    "\n",
    "print(\"Fecha mínima:\", fecha_min)\n",
    "print(\"Fecha máxima:\", fecha_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756c7fde-2374-47af-9ed8-c7138c3581ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las fechas de inicio y fin\n",
    "fecha_inicio = pd.to_datetime('2023-06-24 00:00:00')\n",
    "fecha_fin = pd.to_datetime('2025-04-08 00:00:00')\n",
    "\n",
    "# Filtrar el DataFrame para que solo contenga registros entre fecha_inicio y fecha_fin\n",
    "df_transformadores = df_transformadores[(df_transformadores['Fecha y hora'] >= fecha_inicio) & (df_transformadores['Fecha y hora'] <= fecha_fin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ed6512-d2f4-4d63-87ee-d18fffecefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62785 entries, 86782 to 149566\n",
      "Data columns (total 13 columns):\n",
      " #   Column                                                          Non-Null Count  Dtype         \n",
      "---  ------                                                          --------------  -----         \n",
      " 0   Fecha y hora                                                    62785 non-null  datetime64[ns]\n",
      " 1   SE_San_Rafael.Trf_San_Rafael_T1 Potencia activa media (kW)      62785 non-null  float64       \n",
      " 2   SE_San_Rafael.Trf_San_Rafael_T1 Potencia reactiva media (kVAr)  62785 non-null  float64       \n",
      " 3   SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)      62785 non-null  float64       \n",
      " 4   SE_San_Rafael.Trf_San_Rafael_T3 Potencia reactiva media (kVAr)  62785 non-null  float64       \n",
      " 5   SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)      62785 non-null  float64       \n",
      " 6   SE_San_Felipe.Trf_San_Felipe_T1 Potencia reactiva media (kVAr)  62785 non-null  float64       \n",
      " 7   SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)      62785 non-null  float64       \n",
      " 8   SE_San_Felipe.Trf_San_Felipe_T2 Potencia reactiva media (kVAr)  62785 non-null  float64       \n",
      " 9   SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)            62785 non-null  float64       \n",
      " 10  SE_Concon.Trf_Concon_T2_2 Potencia reactiva media (kVAr)        62785 non-null  float64       \n",
      " 11  estacion                                                        62785 non-null  object        \n",
      " 12  estacion_2                                                      62785 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(10), object(2)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_transformadores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaebaa3f-2843-4c17-9f5d-330314191687",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Leer data meteorologica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "770456b2-0792-45ae-a489-a6ec1f276113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time</th>\n",
       "      <th>Estacion</th>\n",
       "      <th>Temp - °C</th>\n",
       "      <th>Velocidad del Viento Media - km/h</th>\n",
       "      <th>Hum - %</th>\n",
       "      <th>Barómetro - mb</th>\n",
       "      <th>Lluvia - mm</th>\n",
       "      <th>Días-grado de enfriamiento</th>\n",
       "      <th>Días-grado de calentamiento</th>\n",
       "      <th>Estacion_Anual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-24 00:00:00</td>\n",
       "      <td>Los Andes</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>invierno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-24 00:00:00</td>\n",
       "      <td>Baron</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1018.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>invierno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-24 00:00:00</td>\n",
       "      <td>Curauma</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1020.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>invierno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-24 00:00:00</td>\n",
       "      <td>La Cruz</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1020.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>invierno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-24 00:15:00</td>\n",
       "      <td>Curauma</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1020.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038</td>\n",
       "      <td>invierno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date & Time   Estacion  Temp - °C  \\\n",
       "0  2023-06-24 00:00:00  Los Andes       12.0   \n",
       "1  2023-06-24 00:00:00      Baron       16.0   \n",
       "2  2023-06-24 00:00:00    Curauma       15.0   \n",
       "3  2023-06-24 00:00:00    La Cruz       17.0   \n",
       "4  2023-06-24 00:15:00    Curauma       15.0   \n",
       "\n",
       "   Velocidad del Viento Media - km/h  Hum - %  Barómetro - mb  Lluvia - mm  \\\n",
       "0                                0.0     93.0          1023.0          0.6   \n",
       "1                                6.0     92.0          1018.4          0.3   \n",
       "2                                0.0     98.0          1020.8          0.0   \n",
       "3                                1.0     84.0          1020.5          0.0   \n",
       "4                                1.0     98.0          1020.6          0.3   \n",
       "\n",
       "   Días-grado de enfriamiento  Días-grado de calentamiento Estacion_Anual  \n",
       "0                         0.0                        0.123       invierno  \n",
       "1                         0.0                        0.021       invierno  \n",
       "2                         0.0                        0.039       invierno  \n",
       "3                         0.0                        0.012       invierno  \n",
       "4                         0.0                        0.038       invierno  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lee el archivo CSV con separador ;\n",
    "df_meteorologico = pd.read_csv('df_meteorologico.csv', sep=';')\n",
    "\n",
    "# Muestra las primeras filas del DataFrame\n",
    "df_meteorologico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abd597c1-eaf0-4815-bf96-8f0212d2ad01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha mínima: 2023-06-24 00:00:00\n",
      "Fecha máxima: 2025-04-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que la columna 'Fecha y hora' sea de tipo datetime\n",
    "df_meteorologico['Date & Time'] = pd.to_datetime(df_meteorologico['Date & Time'])\n",
    "\n",
    "# Mostrar la fecha mínima y máxima\n",
    "fecha_min = df_meteorologico['Date & Time'].min()\n",
    "fecha_max = df_meteorologico['Date & Time'].max()\n",
    "\n",
    "print(\"Fecha mínima:\", fecha_min)\n",
    "print(\"Fecha máxima:\", fecha_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49fda8b1-5a4c-4f67-a122-b0ec19225f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 251140 entries, 0 to 251139\n",
      "Data columns (total 10 columns):\n",
      " #   Column                             Non-Null Count   Dtype         \n",
      "---  ------                             --------------   -----         \n",
      " 0   Date & Time                        251140 non-null  datetime64[ns]\n",
      " 1   Estacion                           251140 non-null  object        \n",
      " 2   Temp - °C                          251140 non-null  float64       \n",
      " 3   Velocidad del Viento Media - km/h  251140 non-null  float64       \n",
      " 4   Hum - %                            251140 non-null  float64       \n",
      " 5   Barómetro - mb                     251140 non-null  float64       \n",
      " 6   Lluvia - mm                        251140 non-null  float64       \n",
      " 7   Días-grado de enfriamiento         251140 non-null  float64       \n",
      " 8   Días-grado de calentamiento        251140 non-null  float64       \n",
      " 9   Estacion_Anual                     241932 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(7), object(2)\n",
      "memory usage: 19.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_meteorologico.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ff1990f-b0d6-4ad4-8650-214f676cb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPEO_TRANSF_MEO = {\n",
    "    'SE_Concon.Trf_Concon_T2_2 Potencia activa media (kW)': 'Baron',\n",
    "    'SE_San_Rafael.Trf_San_Rafael_T1 Potencia activa media (kW)': 'Los Andes',\n",
    "    'SE_San_Rafael.Trf_San_Rafael_T3 Potencia activa media (kW)': 'Los Andes',\n",
    "    'SE_San_Felipe.Trf_San_Felipe_T2 Potencia activa media (kW)': 'Los Andes',\n",
    "    'SE_San_Felipe.Trf_San_Felipe_T1 Potencia activa media (kW)': 'Los Andes',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cee4ed-6546-4b1a-bf91-c22f0258524c",
   "metadata": {},
   "source": [
    "# **Instalación de librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "803a797f-bb24-4055-bd30-ec15fbccad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prophet in /opt/anaconda3/lib/python3.12/site-packages (1.1.6)\n",
      "Requirement already satisfied: cmdstanpy>=1.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (1.2.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (3.9.2)\n",
      "Requirement already satisfied: pandas>=1.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (2.2.2)\n",
      "Requirement already satisfied: holidays<1,>=0.25 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (0.70)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (4.66.5)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.12/site-packages (from prophet) (6.5.2)\n",
      "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from holidays<1,>=0.25->prophet) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=2.0.0->prophet) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.4->prophet) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.0.4->prophet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68f207fb-81a9-4439-b5b4-b0f901fb2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352aed6-089f-47d1-a5f3-69342cd75381",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Modelos Invierten Q**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3103e010-4358-4129-9c8f-5e3f97b17d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_mape(y_true, y_pred, min_denominador=1.0):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true >= min_denominador\n",
    "    if np.any(mask):\n",
    "        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804910b8-e999-46e2-9b54-a318b29c141f",
   "metadata": {},
   "source": [
    "# Estación + meteo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b2cfc37-1a9b-4d4d-bb3c-37e862fe0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_modelo_prophet_parametros_estacion_manual(df, columna, fecha_inicio_test, df_meteorologico, nombre_estacion, mostrar_grafico=False, ruta_guardado=None):\n",
    "    # Preparar datos\n",
    "    df_prophet = df[['Fecha y hora', columna, 'estacion']].dropna().rename(columns={\n",
    "        'Fecha y hora': 'ds', columna: 'y'\n",
    "    })\n",
    "    df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "\n",
    "    # Añadir temperatura desde df_meteorologico\n",
    "    df_meteorologico_estacion = df_meteorologico[df_meteorologico['Estacion'] == nombre_estacion].copy()\n",
    "    df_meteorologico_estacion['Date & Time'] = pd.to_datetime(df_meteorologico_estacion['Date & Time'])\n",
    "    df_prophet = df_prophet.merge(\n",
    "        df_meteorologico_estacion[['Date & Time', 'Temp - °C']],\n",
    "        left_on='ds',\n",
    "        right_on='Date & Time',\n",
    "        how='left'\n",
    "    ).drop(columns=['Date & Time'])\n",
    "    df_prophet.rename(columns={'Temp - °C': 'temp'}, inplace=True)\n",
    "\n",
    "    # Dummies de estación (primavera, verano...)\n",
    "    dummies_estacion = pd.get_dummies(df_prophet['estacion'], prefix='estacion')\n",
    "    df_prophet = pd.concat([df_prophet, dummies_estacion], axis=1)\n",
    "    regresores = ['temp'] + dummies_estacion.columns.tolist()\n",
    "\n",
    "\n",
    "    # Fechas clave\n",
    "    fecha_inicio_test = pd.to_datetime(fecha_inicio_test)\n",
    "    fecha_fin_test = fecha_inicio_test + pd.Timedelta(days=7)\n",
    "    fecha_fin_train_eval = fecha_inicio_test - pd.Timedelta(minutes=15)\n",
    "    fecha_inicio_train_eval = fecha_fin_train_eval - pd.Timedelta(weeks=1) + pd.Timedelta(minutes=15)\n",
    "\n",
    "    # Filtrar conjuntos\n",
    "    df_test = df_prophet[(df_prophet['ds'] >= fecha_inicio_test) & (df_prophet['ds'] < fecha_fin_test)].copy()\n",
    "    df_train_eval = df_prophet[(df_prophet['ds'] >= fecha_inicio_train_eval) & (df_prophet['ds'] < fecha_fin_train_eval)].copy()\n",
    "    df_train = df_prophet[df_prophet['ds'] <= fecha_fin_train_eval].copy()\n",
    "\n",
    "    # Crear y configurar el modelo\n",
    "    m = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        yearly_seasonality=False,\n",
    "        n_changepoints=50\n",
    "    )\n",
    "    m.add_seasonality(name='daily_hour', period=1, fourier_order=20)\n",
    "    m.add_seasonality(name='weekly_custom', period=7, fourier_order=30)\n",
    "    m.add_seasonality(name='yearly_custom', period=365.25, fourier_order=10)\n",
    "\n",
    "    for reg in regresores:\n",
    "        m.add_regressor(reg)\n",
    "\n",
    "    # Entrenamiento\n",
    "    m.fit(df_train[['ds', 'y'] + regresores])\n",
    "\n",
    "    # Preparar fechas futuras\n",
    "    future = pd.concat([df_train_eval[['ds']], df_test[['ds']]]).drop_duplicates().sort_values('ds')\n",
    "    future = future.merge(df_prophet[['ds'] + regresores], on='ds', how='left')\n",
    "\n",
    "    # Predicción\n",
    "    forecast = m.predict(future)\n",
    "    forecast_train_eval = forecast[forecast['ds'].isin(df_train_eval['ds'])]\n",
    "    forecast_test = forecast[forecast['ds'].isin(df_test['ds'])]\n",
    "\n",
    "    df_train_eval = df_train_eval.set_index('ds').join(forecast_train_eval.set_index('ds')[['yhat']], how='inner')\n",
    "    df_test_eval = df_test.set_index('ds').join(forecast_test.set_index('ds')[['yhat']], how='inner')\n",
    "\n",
    "    # Métricas\n",
    "    mae_train = mean_absolute_error(df_train_eval['y'], df_train_eval['yhat'])\n",
    "    rmse_train = np.sqrt(mean_squared_error(df_train_eval['y'], df_train_eval['yhat']))\n",
    "    mape_train = calcular_mape(df_train_eval['y'], df_train_eval['yhat'])\n",
    "\n",
    "    mae_test = mean_absolute_error(df_test_eval['y'], df_test_eval['yhat'])\n",
    "    rmse_test = np.sqrt(mean_squared_error(df_test_eval['y'], df_test_eval['yhat']))\n",
    "    mape_test = calcular_mape(df_test_eval['y'], df_test_eval['yhat'])\n",
    "\n",
    "    # Gráfico opcional\n",
    "    if mostrar_grafico or ruta_guardado:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df_test_eval.index, df_test_eval['y'], label='Real')\n",
    "        plt.plot(df_test_eval.index, df_test_eval['yhat'], label='Predicción')\n",
    "        plt.title(f'{columna} [{fecha_inicio_test.date()}]')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if ruta_guardado:\n",
    "            plt.savefig(ruta_guardado)\n",
    "        if mostrar_grafico:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    # Armar resultados de métricas\n",
    "    resultados_metricas = {\n",
    "        'transformador': columna,\n",
    "        'fecha_inicio_test': fecha_inicio_test.date(),\n",
    "        'MAE_train': mae_train, 'RMSE_train': rmse_train, 'MAPE_train': mape_train,\n",
    "        'MAE_test': mae_test, 'RMSE_test': rmse_test, 'MAPE_test': mape_test\n",
    "    }\n",
    "\n",
    "    # Preparar df_test_eval para exportar\n",
    "    df_test_eval = df_test_eval.reset_index()\n",
    "    df_test_eval['transformador'] = columna\n",
    "    df_test_eval = df_test_eval.rename(columns={'y': 'real', 'yhat': 'predicho'})\n",
    "\n",
    "    return resultados_metricas, df_test_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cd66c93-6a29-41e6-bddb-a984e257e484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'primavera': [Timestamp('2024-11-25 00:00:00'), Timestamp('2024-11-04 00:00:00'), Timestamp('2024-10-14 00:00:00'), Timestamp('2024-09-23 00:00:00')], 'verano': [Timestamp('2025-02-24 00:00:00'), Timestamp('2025-02-03 00:00:00'), Timestamp('2025-01-13 00:00:00'), Timestamp('2024-12-23 00:00:00')], 'otoño': [Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-03 00:00:00'), Timestamp('2024-03-17 00:00:00')], 'invierno': [Timestamp('2024-08-26 00:00:00'), Timestamp('2024-08-05 00:00:00'), Timestamp('2024-07-15 00:00:00')]}\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from pandas import Timestamp\n",
    "\n",
    "\n",
    "# Define estaciones según hemisferio sur\n",
    "estaciones_meses = {\n",
    "    'primavera': [9, 10, 11],\n",
    "    'verano': [12, 1, 2],\n",
    "    'otoño': [3, 4, 5],\n",
    "    'invierno': [6, 7, 8]\n",
    "}\n",
    "\n",
    "# Fechas límites\n",
    "inicio_data = pd.to_datetime(\"2023-06-24\")\n",
    "fin_data = pd.to_datetime(\"2025-04-02\")\n",
    "duracion_entrenamiento = timedelta(weeks=52)\n",
    "gap = timedelta(weeks=2)\n",
    "duracion_test = timedelta(days=7)\n",
    "\n",
    "# Extraer fechas candidatas desde la columna 'Fecha y hora'\n",
    "fechas_disponibles = pd.Series(pd.to_datetime(df_transformadores[\"Fecha y hora\"].dropna().unique()))\n",
    "fechas_disponibles = fechas_disponibles.sort_values()\n",
    "\n",
    "\n",
    "def seleccionar_fechas_test(estacion, meses_objetivo, cantidad=4, min_dias_entre_fechas=21):\n",
    "    candidatos = []\n",
    "    fechas_filtradas = fechas_disponibles[\n",
    "        (fechas_disponibles.dt.weekday == 0) &  # lunes\n",
    "        (fechas_disponibles.dt.hour == 0) &     # a las 00:00\n",
    "        (fechas_disponibles.dt.minute == 0) &   # y minuto 00\n",
    "        (fechas_disponibles.dt.month.isin(meses_objetivo))\n",
    "    ].sort_values(ascending=False)  # orden descendente\n",
    "\n",
    "    for fecha in fechas_filtradas:\n",
    "        # Validar que haya suficiente data de entrenamiento\n",
    "        inicio_entrenamiento = fecha - gap - duracion_entrenamiento\n",
    "        if inicio_entrenamiento < inicio_data or (fecha + duracion_test) > fin_data:\n",
    "            continue\n",
    "        # Evitar semanas muy cercanas entre sí\n",
    "        if any(abs((fecha - f).days) < min_dias_entre_fechas for f in candidatos):\n",
    "            continue\n",
    "        candidatos.append(fecha)\n",
    "        if len(candidatos) == cantidad:\n",
    "            break\n",
    "    return candidatos\n",
    "\n",
    "\n",
    "# Armar el diccionario completo\n",
    "fechas_test_por_estacion = {\n",
    "    estacion: seleccionar_fechas_test(estacion, meses)\n",
    "    for estacion, meses in estaciones_meses.items()\n",
    "}\n",
    "fechas_test_por_estacion['otoño'].append(Timestamp('2024-03-17 00:00:00'))\n",
    "\n",
    "print(fechas_test_por_estacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "102ba371-260b-421e-9a84-5e7d498e024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:11:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:11:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:11:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:12:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:12:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:13:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:13:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:13:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:13:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:13:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:14:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:14:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:14:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:15:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:15:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:15:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:15:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:16:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:16:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:17:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:17:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:17:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:17:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:17:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:17:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:18:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:18:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:18:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:18:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:18:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:18:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:18:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:19:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:19:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:19:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:19:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:19:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:19:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:20:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:20:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:20:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:20:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:20:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:21:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:21:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:21:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:21:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:22:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:22:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:22:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:22:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:22:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:22:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:22:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:22:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:22:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:22:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:23:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:23:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:23:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:23:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:23:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:23:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:24:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:24:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:24:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:24:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:25:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:25:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:25:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:25:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:27:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:27:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:27:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:27:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:27:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:27:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:27:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:27:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:27:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:27:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:28:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:28:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:28:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:28:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:28:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:28:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:29:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:29:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:29:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:29:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:30:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:30:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:30:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:30:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:30:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:30:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:31:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:31:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:32:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:32:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:32:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:32:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:32:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:32:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:32:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:32:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:32:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:32:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:33:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:33:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:33:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:33:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:33:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:33:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:34:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:34:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:34:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:34:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:34:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:34:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:35:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:35:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:35:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:35:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:36:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:36:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:36:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:36:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:36:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:36:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:36:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:36:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:37:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:37:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:37:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta para guardar gráficos si no existe\n",
    "carpeta_graficos = \"graficos_resultados_1_año\"\n",
    "os.makedirs(carpeta_graficos, exist_ok=True)\n",
    "\n",
    "# Listas para acumular resultados\n",
    "resultados = []\n",
    "predicciones_todas = []\n",
    "\n",
    "# Bucle principal\n",
    "for transformador in transformador_columnas_p:\n",
    "    # Obtener el nombre de la estación según tu mapeo\n",
    "    nombre_estacion = MAPEO_TRANSF_MEO.get(transformador)\n",
    "    if nombre_estacion is None:\n",
    "        print(f\"Aviso: No se encontró estación meteorológica para {transformador}. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    for estacion, fechas in fechas_test_por_estacion.items():\n",
    "        for fecha in fechas:\n",
    "            try:\n",
    "                # Formatear fecha para nombre de archivo\n",
    "                fecha_str = fecha.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "                # Limpiar nombre del transformador para archivo\n",
    "                nombre_archivo = (\n",
    "                    transformador.replace(\" \", \"_\")\n",
    "                    .replace(\".\", \"_\")\n",
    "                    .replace(\"(\", \"\")\n",
    "                    .replace(\")\", \"\")\n",
    "                    .replace(\"/\", \"_\")\n",
    "                )\n",
    "                ruta_grafico = os.path.join(carpeta_graficos, f\"{nombre_archivo}_{fecha_str}.png\")\n",
    "\n",
    "                # Ejecutar modelo\n",
    "                resultado, predicciones_test = ejecutar_modelo_prophet_parametros_estacion_manual(\n",
    "                    df=df_transformadores, \n",
    "                    columna=transformador, \n",
    "                    fecha_inicio_test=fecha, \n",
    "                    df_meteorologico=df_meteorologico, \n",
    "                    nombre_estacion=nombre_estacion,\n",
    "                    mostrar_grafico=False,\n",
    "                    ruta_guardado=ruta_grafico\n",
    "                )\n",
    "\n",
    "                # Agregar información contextual\n",
    "                resultado[\"transformador\"] = transformador\n",
    "                resultado[\"fecha_test\"] = fecha\n",
    "                resultado[\"estacion\"] = estacion\n",
    "                resultado[\"estacion_meteorologica\"] = nombre_estacion\n",
    "                resultados.append(resultado)\n",
    "\n",
    "                predicciones_test[\"fecha_test\"] = fecha\n",
    "                predicciones_test[\"estacion\"] = estacion\n",
    "                predicciones_test[\"estacion_meteorologica\"] = nombre_estacion\n",
    "                predicciones_todas.append(predicciones_test)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en {transformador}, {fecha}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7773209f-4dd9-43a5-a25e-518dbd2b0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       transformador fecha_inicio_test  \\\n",
      "0  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-11-25   \n",
      "1  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-11-04   \n",
      "2  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-10-14   \n",
      "3  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-09-23   \n",
      "4  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2025-02-24   \n",
      "\n",
      "   MAE_train  RMSE_train  MAPE_train  MAE_test  RMSE_test  MAPE_test  \\\n",
      "0   2.704492    3.059306   31.493771  2.845195   3.498066  28.143796   \n",
      "1   3.064260    3.785921   40.559061  3.444456   4.184354  38.863537   \n",
      "2   2.405212    3.415025   28.454387  3.567108   4.934551  26.563098   \n",
      "3   3.580866    4.656173   45.394591  4.273529   4.876564  56.603213   \n",
      "4   1.890869    2.382893   27.413371  2.919423   3.556757  51.457918   \n",
      "\n",
      "  fecha_test   estacion estacion_meteorologica  \n",
      "0 2024-11-25  primavera              Los Andes  \n",
      "1 2024-11-04  primavera              Los Andes  \n",
      "2 2024-10-14  primavera              Los Andes  \n",
      "3 2024-09-23  primavera              Los Andes  \n",
      "4 2025-02-24     verano              Los Andes  \n",
      "                   ds       real   estacion  temp  estacion_invierno  \\\n",
      "0 2024-11-25 00:00:00  11.080289  primavera  18.0              False   \n",
      "1 2024-11-25 00:15:00  10.758238  primavera  18.0              False   \n",
      "2 2024-11-25 00:30:00  10.307382  primavera  18.0              False   \n",
      "3 2024-11-25 00:45:00  10.033526  primavera  18.0              False   \n",
      "4 2024-11-25 01:00:00   9.729985  primavera  18.0              False   \n",
      "\n",
      "   estacion_otoño  estacion_primavera  estacion_verano  predicho  \\\n",
      "0           False                True            False  8.027535   \n",
      "1           False                True            False  7.483747   \n",
      "2           False                True            False  6.929171   \n",
      "3           False                True            False  6.386537   \n",
      "4           False                True            False  5.885082   \n",
      "\n",
      "                                       transformador fecha_test  \\\n",
      "0  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-11-25   \n",
      "1  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-11-25   \n",
      "2  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-11-25   \n",
      "3  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-11-25   \n",
      "4  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-11-25   \n",
      "\n",
      "  estacion_meteorologica  \n",
      "0              Los Andes  \n",
      "1              Los Andes  \n",
      "2              Los Andes  \n",
      "3              Los Andes  \n",
      "4              Los Andes  \n"
     ]
    }
   ],
   "source": [
    "# Convertir a DataFrame los resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_predicciones = pd.concat(predicciones_todas, ignore_index=True)\n",
    "\n",
    "# Mostrar (o guardar si lo deseas)\n",
    "print(df_resultados.head())\n",
    "print(df_predicciones.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bac03220-725a-4601-857e-9bd57a1d381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b8b0da2-2c28-4d1b-9876-dbd548de9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"resultados_prophet_completos_invierten_P_sin_gap.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_resultados.to_excel(writer, sheet_name=\"Metricas\", index=False)\n",
    "    df_predicciones.to_excel(writer, sheet_name=\"Predicciones\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61d39a-d9cf-4aed-bd3c-86c186f74a3d",
   "metadata": {},
   "source": [
    "## nuevos test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "099b6ed2-e9ec-48cc-9ed5-52dc95bc3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet   # <-- paquete actualizado\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calcular_mape(y_true, y_pred):\n",
    "    \"\"\"Calcula el MAPE evitando divisiones por cero.\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero = y_true != 0\n",
    "    if non_zero.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[non_zero] - y_pred[non_zero]) / y_true[non_zero])) * 100\n",
    "\n",
    "\n",
    "def ejecutar_modelo_prophet_cdd_pmgd_proxy(\n",
    "    df,\n",
    "    columna,\n",
    "    fecha_inicio_test,\n",
    "    df_meteorologico,\n",
    "    nombre_estacion,\n",
    "    col_temp=\"Temp - °C\",\n",
    "    col_cdd=\"Días-grado de enfriamiento\",\n",
    "    col_hdd=\"Días-grado de calentamiento\",\n",
    "    mostrar_grafico=False,\n",
    "    ruta_guardado=None,\n",
    "):\n",
    "    \"\"\"Versión de Prophet con:\n",
    "    - Lectura de CDD/HDD desde df_meteorologico.\n",
    "    - Proxy solar estacional ajustado por estación del año en Chile.\n",
    "    - Seasonality multiplicativa y Fourier daily/weekly/yearly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preparar datos base\n",
    "    df_prophet = df[[\"Fecha y hora\", columna, \"estacion\"]].dropna().rename(columns={\n",
    "        \"Fecha y hora\": \"ds\", columna: \"y\"\n",
    "    })\n",
    "    df_prophet[\"ds\"] = pd.to_datetime(df_prophet[\"ds\"])\n",
    "\n",
    "    # Preparar meteo (con CDD y HDD incluidos)\n",
    "    meteo = df_meteorologico[df_meteorologico[\"Estacion\"] == nombre_estacion].copy()\n",
    "    meteo[\"Date & Time\"] = pd.to_datetime(meteo[\"Date & Time\"])\n",
    "    meteo = meteo[[\"Date & Time\", col_cdd, col_hdd]].rename(columns={\n",
    "        \"Date & Time\": \"ds\",\n",
    "        col_cdd: \"CDD\",\n",
    "        col_hdd: \"HDD\"\n",
    "    })\n",
    "    meteo[\"CDD2\"] = meteo[\"CDD\"] ** 2\n",
    "    meteo[\"HDD2\"] = meteo[\"HDD\"] ** 2\n",
    "\n",
    "    # Merge a df_prophet\n",
    "    df_prophet = df_prophet.merge(meteo, on=\"ds\", how=\"left\")\n",
    "\n",
    "    # Proxy solar estacional\n",
    "    def _pmgd_prob(row):\n",
    "        hora_decimal = row[\"ds\"].hour + row[\"ds\"].minute / 60.0\n",
    "        est = row[\"estacion\"].lower()\n",
    "        if est == \"verano\":\n",
    "            return int(8 <= hora_decimal <= 19)\n",
    "        elif est == \"invierno\":\n",
    "            return int(10 <= hora_decimal <= 17)\n",
    "        else:\n",
    "            return int(9 <= hora_decimal <= 18)\n",
    "\n",
    "    df_prophet[\"pmgd_horario_probable\"] = df_prophet.apply(_pmgd_prob, axis=1)\n",
    "\n",
    "    # Dummies estación\n",
    "    dummies_estacion = pd.get_dummies(df_prophet[\"estacion\"], prefix=\"estacion\")\n",
    "    df_prophet = pd.concat([df_prophet, dummies_estacion], axis=1)\n",
    "\n",
    "    regresores = [\"CDD\", \"HDD\", \"CDD2\", \"HDD2\", \"pmgd_horario_probable\"] + dummies_estacion.columns.tolist()\n",
    "\n",
    "    # Fechas\n",
    "    fecha_inicio_test = pd.to_datetime(fecha_inicio_test)\n",
    "    fecha_fin_test = fecha_inicio_test + pd.Timedelta(days=7)\n",
    "    fecha_fin_train_eval = fecha_inicio_test - pd.Timedelta(minutes=15)\n",
    "    fecha_inicio_train_eval = fecha_fin_train_eval - pd.Timedelta(weeks=1) + pd.Timedelta(minutes=15)\n",
    "\n",
    "    df_test = df_prophet[(df_prophet[\"ds\"] >= fecha_inicio_test) & (df_prophet[\"ds\"] < fecha_fin_test)].copy()\n",
    "    df_train_eval = df_prophet[(df_prophet[\"ds\"] >= fecha_inicio_train_eval) & (df_prophet[\"ds\"] < fecha_fin_train_eval)].copy()\n",
    "    df_train = df_prophet[df_prophet[\"ds\"] <= fecha_fin_train_eval].copy()\n",
    "\n",
    "    # Prophet\n",
    "    m = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        yearly_seasonality=False,\n",
    "        seasonality_mode=\"multiplicative\",\n",
    "        n_changepoints=80,\n",
    "        changepoint_prior_scale=0.1,\n",
    "        seasonality_prior_scale=10,\n",
    "    )\n",
    "\n",
    "    m.add_seasonality(name=\"daily_15m\", period=1, fourier_order=48)\n",
    "    m.add_seasonality(name=\"weekly_custom\", period=7, fourier_order=30)\n",
    "    m.add_seasonality(name=\"yearly_custom\", period=365.25, fourier_order=10)\n",
    "\n",
    "    for reg in regresores:\n",
    "        m.add_regressor(reg)\n",
    "\n",
    "    m.fit(df_train[[\"ds\", \"y\"] + regresores])\n",
    "\n",
    "    future = pd.concat([df_train_eval[[\"ds\"]], df_test[[\"ds\"]]]).drop_duplicates().sort_values(\"ds\")\n",
    "    future = future.merge(df_prophet[[\"ds\"] + regresores], on=\"ds\", how=\"left\")\n",
    "\n",
    "    forecast = m.predict(future)\n",
    "    forecast_train_eval = forecast[forecast[\"ds\"].isin(df_train_eval[\"ds\"])]\n",
    "    forecast_test = forecast[forecast[\"ds\"].isin(df_test[\"ds\"])]\n",
    "\n",
    "    df_train_eval = df_train_eval.set_index(\"ds\").join(\n",
    "        forecast_train_eval.set_index(\"ds\")[[\"yhat\"]], how=\"inner\"\n",
    "    )\n",
    "    df_test_eval = df_test.set_index(\"ds\").join(\n",
    "        forecast_test.set_index(\"ds\")[[\"yhat\"]], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    mae_train = mean_absolute_error(df_train_eval[\"y\"], df_train_eval[\"yhat\"])\n",
    "    rmse_train = np.sqrt(mean_squared_error(df_train_eval[\"y\"], df_train_eval[\"yhat\"]))\n",
    "    mape_train = calcular_mape(df_train_eval[\"y\"], df_train_eval[\"yhat\"])\n",
    "\n",
    "    mae_test = mean_absolute_error(df_test_eval[\"y\"], df_test_eval[\"yhat\"])\n",
    "    rmse_test = np.sqrt(mean_squared_error(df_test_eval[\"y\"], df_test_eval[\"yhat\"]))\n",
    "    mape_test = calcular_mape(df_test_eval[\"y\"], df_test_eval[\"yhat\"])\n",
    "\n",
    "    resultados_metricas = {\n",
    "        \"transformador\": columna,\n",
    "        \"fecha_inicio_test\": fecha_inicio_test.date(),\n",
    "        \"MAE_train\": mae_train,\n",
    "        \"RMSE_train\": rmse_train,\n",
    "        \"MAPE_train\": mape_train,\n",
    "        \"MAE_test\": mae_test,\n",
    "        \"RMSE_test\": rmse_test,\n",
    "        \"MAPE_test\": mape_test,\n",
    "    }\n",
    "\n",
    "    if mostrar_grafico or ruta_guardado:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df_test_eval.index, df_test_eval[\"y\"], label=\"Real\")\n",
    "        plt.plot(df_test_eval.index, df_test_eval[\"yhat\"], label=\"Predicción\")\n",
    "        plt.title(f\"{columna} [{fecha_inicio_test.date()}]\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if ruta_guardado:\n",
    "            plt.savefig(ruta_guardado)\n",
    "        if mostrar_grafico:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    df_test_eval = df_test_eval.reset_index()\n",
    "    df_test_eval[\"transformador\"] = columna\n",
    "    df_test_eval = df_test_eval.rename(columns={\"y\": \"real\", \"yhat\": \"predicho\"})\n",
    "\n",
    "    return resultados_metricas, df_test_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "184dc913-8b4e-4781-b64d-f370e2a8becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'primavera': [Timestamp('2024-11-25 00:00:00'), Timestamp('2024-11-04 00:00:00'), Timestamp('2024-10-14 00:00:00'), Timestamp('2024-09-23 00:00:00')], 'verano': [Timestamp('2025-02-24 00:00:00'), Timestamp('2025-02-03 00:00:00'), Timestamp('2025-01-13 00:00:00'), Timestamp('2024-12-23 00:00:00')], 'otoño': [Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-03 00:00:00'), Timestamp('2024-03-17 00:00:00')], 'invierno': [Timestamp('2024-08-26 00:00:00'), Timestamp('2024-08-05 00:00:00'), Timestamp('2024-07-15 00:00:00')]}\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from pandas import Timestamp\n",
    "\n",
    "\n",
    "# Define estaciones según hemisferio sur\n",
    "estaciones_meses = {\n",
    "    'primavera': [9, 10, 11],\n",
    "    'verano': [12, 1, 2],\n",
    "    'otoño': [3, 4, 5],\n",
    "    'invierno': [6, 7, 8]\n",
    "}\n",
    "\n",
    "# Fechas límites\n",
    "inicio_data = pd.to_datetime(\"2023-06-24\")\n",
    "fin_data = pd.to_datetime(\"2025-04-02\")\n",
    "duracion_entrenamiento = timedelta(weeks=52)\n",
    "gap = timedelta(weeks=2)\n",
    "duracion_test = timedelta(days=7)\n",
    "\n",
    "# Extraer fechas candidatas desde la columna 'Fecha y hora'\n",
    "fechas_disponibles = pd.Series(pd.to_datetime(df_transformadores[\"Fecha y hora\"].dropna().unique()))\n",
    "fechas_disponibles = fechas_disponibles.sort_values()\n",
    "\n",
    "\n",
    "def seleccionar_fechas_test(estacion, meses_objetivo, cantidad=4, min_dias_entre_fechas=21):\n",
    "    candidatos = []\n",
    "    fechas_filtradas = fechas_disponibles[\n",
    "        (fechas_disponibles.dt.weekday == 0) &  # lunes\n",
    "        (fechas_disponibles.dt.hour == 0) &     # a las 00:00\n",
    "        (fechas_disponibles.dt.minute == 0) &   # y minuto 00\n",
    "        (fechas_disponibles.dt.month.isin(meses_objetivo))\n",
    "    ].sort_values(ascending=False)  # orden descendente\n",
    "\n",
    "    for fecha in fechas_filtradas:\n",
    "        # Validar que haya suficiente data de entrenamiento\n",
    "        inicio_entrenamiento = fecha - gap - duracion_entrenamiento\n",
    "        if inicio_entrenamiento < inicio_data or (fecha + duracion_test) > fin_data:\n",
    "            continue\n",
    "        # Evitar semanas muy cercanas entre sí\n",
    "        if any(abs((fecha - f).days) < min_dias_entre_fechas for f in candidatos):\n",
    "            continue\n",
    "        candidatos.append(fecha)\n",
    "        if len(candidatos) == cantidad:\n",
    "            break\n",
    "    return candidatos\n",
    "\n",
    "\n",
    "# Armar el diccionario completo\n",
    "fechas_test_por_estacion = {\n",
    "    estacion: seleccionar_fechas_test(estacion, meses)\n",
    "    for estacion, meses in estaciones_meses.items()\n",
    "}\n",
    "fechas_test_por_estacion['otoño'].append(Timestamp('2024-03-17 00:00:00'))\n",
    "\n",
    "print(fechas_test_por_estacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db1812f0-fe90-44cc-bc81-232d1dd1283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:40:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:41:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:41:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:41:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:41:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:42:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:42:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:43:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:43:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:45:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:45:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:46:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:46:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:48:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:48:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:49:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:50:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:52:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:52:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:53:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:53:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:53:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:53:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:55:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:55:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:56:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:56:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:57:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:57:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:59:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:59:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:02:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:02:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:03:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:03:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:04:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:04:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:05:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:05:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:06:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:06:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:08:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:08:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:09:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:09:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:10:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:10:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:11:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:11:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:12:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:12:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:12:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:13:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:13:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:13:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:14:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:15:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:16:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:16:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:17:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:17:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:18:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:18:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:20:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:20:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:21:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:21:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:22:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:22:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:24:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:24:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:25:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:25:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:26:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:26:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:27:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:27:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:27:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:28:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:28:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:29:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:29:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:31:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:31:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:32:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:32:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:33:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:33:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:35:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:35:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:37:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:37:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:38:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:38:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:39:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:39:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:41:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:41:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:43:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:43:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:44:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:44:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:45:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:45:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:46:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:46:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:47:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:47:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:49:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:49:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:51:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:51:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:53:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:53:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:55:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:55:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:57:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:57:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:59:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:59:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:00:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:00:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:02:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:02:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:04:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:04:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:05:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:05:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:07:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:07:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:09:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:09:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:10:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:10:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:12:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta para guardar gráficos si no existe\n",
    "carpeta_graficos = \"graficos_resultados_1_año\"\n",
    "os.makedirs(carpeta_graficos, exist_ok=True)\n",
    "\n",
    "# Listas para acumular resultados\n",
    "resultados = []\n",
    "predicciones_todas = []\n",
    "\n",
    "# Bucle principal\n",
    "for transformador in transformador_columnas_p:\n",
    "    # Obtener el nombre de la estación según tu mapeo\n",
    "    nombre_estacion = MAPEO_TRANSF_MEO.get(transformador)\n",
    "    if nombre_estacion is None:\n",
    "        print(f\"Aviso: No se encontró estación meteorológica para {transformador}. Se omite.\")\n",
    "        continue\n",
    "\n",
    "    for estacion, fechas in fechas_test_por_estacion.items():\n",
    "        for fecha in fechas:\n",
    "            try:\n",
    "                # Formatear fecha para nombre de archivo\n",
    "                fecha_str = fecha.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "                # Limpiar nombre del transformador para archivo\n",
    "                nombre_archivo = (\n",
    "                    transformador.replace(\" \", \"_\")\n",
    "                    .replace(\".\", \"_\")\n",
    "                    .replace(\"(\", \"\")\n",
    "                    .replace(\")\", \"\")\n",
    "                    .replace(\"/\", \"_\")\n",
    "                )\n",
    "                ruta_grafico = os.path.join(carpeta_graficos, f\"{nombre_archivo}_{fecha_str}.png\")\n",
    "\n",
    "                # Ejecutar modelo\n",
    "                resultado, predicciones_test = ejecutar_modelo_prophet_cdd_pmgd_proxy(\n",
    "                    df=df_transformadores, \n",
    "                    columna=transformador, \n",
    "                    fecha_inicio_test=fecha, \n",
    "                    df_meteorologico=df_meteorologico, \n",
    "                    nombre_estacion=nombre_estacion,\n",
    "                    mostrar_grafico=False,\n",
    "                    ruta_guardado=ruta_grafico\n",
    "                )\n",
    "\n",
    "                # Agregar información contextual\n",
    "                resultado[\"transformador\"] = transformador\n",
    "                resultado[\"fecha_test\"] = fecha\n",
    "                resultado[\"estacion\"] = estacion\n",
    "                resultado[\"estacion_meteorologica\"] = nombre_estacion\n",
    "                resultados.append(resultado)\n",
    "\n",
    "                predicciones_test[\"fecha_test\"] = fecha\n",
    "                predicciones_test[\"estacion\"] = estacion\n",
    "                predicciones_test[\"estacion_meteorologica\"] = nombre_estacion\n",
    "                predicciones_todas.append(predicciones_test)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en {transformador}, {fecha}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6d00315-b718-4877-8d8e-b0b291c0b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame los resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_predicciones = pd.concat(predicciones_todas, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37bc045f-d397-4398-9e4b-df65fae4e3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x106326a20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1940, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1957, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n",
      "Exception ignored in: <function ZipFile.__del__ at 0x106326a20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1940, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1957, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n",
      "Exception ignored in: <function ZipFile.__del__ at 0x106326a20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1940, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/anaconda3/lib/python3.12/zipfile/__init__.py\", line 1957, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(\"resultados_prophet_completos_invierten_P_sin_gap_mejorado.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_resultados.to_excel(writer, sheet_name=\"Metricas\", index=False)\n",
    "    df_predicciones.to_excel(writer, sheet_name=\"Predicciones\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecd459-82b3-4979-880b-ffce1bce849c",
   "metadata": {},
   "source": [
    "## Prueba de estres con meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a20fc0ea-3abc-4667-a09d-c08e9dbc56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Código cargado y listo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:04:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:05:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:05:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:06:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:06:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:08:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:08:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:11:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:11:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:12:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:12:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:15:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:15:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:17:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:17:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:18:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:18:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:20:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:20:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:21:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:21:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:22:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:22:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:23:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:23:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:25:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:25:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:26:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:26:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:27:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:27:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:28:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:28:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:29:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:29:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:30:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:30:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:31:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:31:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:33:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:33:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:34:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:34:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:36:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:36:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:37:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:37:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:38:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:39:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:39:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:40:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:42:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:42:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:43:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:43:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:44:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:44:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:46:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:46:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:46:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:46:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:49:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:49:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:51:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:51:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:52:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:52:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:54:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:54:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:55:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:56:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:57:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:57:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:59:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:59:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:01:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:01:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:02:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:02:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:04:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:04:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:07:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:07:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:10:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:10:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:14:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:14:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:16:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:16:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:18:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:18:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:19:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:19:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:22:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:22:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:24:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:24:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:26:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:26:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:28:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:28:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:31:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:31:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:33:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:33:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:35:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:35:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:36:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:36:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:39:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:39:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:41:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:41:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:44:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:44:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:46:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:47:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:50:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:50:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:54:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:54:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:56:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:56:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:57:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:57:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "14:59:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "14:59:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:01:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:01:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:03:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:03:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:05:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:05:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:07:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:07:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:10:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:10:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:11:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:11:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:13:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:13:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:16:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:16:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:19:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:19:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:21:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:21:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:24:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:24:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:26:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:28:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:28:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:30:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:30:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:32:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:32:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:33:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:33:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:34:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:34:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:36:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:36:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:38:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:38:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:40:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:41:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:41:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:42:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:42:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:44:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:44:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:45:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:45:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:46:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:46:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:48:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:48:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:49:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:49:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:51:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:51:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:52:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:52:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:53:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:53:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:54:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:54:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:56:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:56:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:57:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:57:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:59:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "15:59:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:01:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:01:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:03:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:03:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:05:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:05:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:06:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:06:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:07:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:07:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:09:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:09:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:10:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:10:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:11:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:11:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:12:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:12:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:14:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:15:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:16:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:16:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:17:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:17:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:18:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:18:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:20:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:20:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:22:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:22:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:24:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:25:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:25:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:26:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:26:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:28:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:28:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:30:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:30:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:32:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:32:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:33:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:33:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:36:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:36:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:38:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:38:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:40:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:40:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:41:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:41:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:43:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:43:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:44:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:44:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:45:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:45:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:46:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:46:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:48:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:48:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:49:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:49:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:51:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:51:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:52:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:53:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:54:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:54:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:55:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:55:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:57:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:57:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:58:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:58:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:59:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:59:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:03:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:03:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:04:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:04:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:05:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:05:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:06:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:06:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:07:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:07:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:08:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:08:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:09:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:09:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:10:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:10:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:11:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:11:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:12:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:12:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:14:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:14:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:15:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:15:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:16:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:16:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:18:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:18:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:19:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:19:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:20:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:20:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:21:36 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Resultados guardados correctamente con estacionalidades y gap (sin límite de 1 año).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "# === RUTAS DE SALIDA ===\n",
    "output_dir = \"resultados_prophet_estacional_gap\"\n",
    "carpeta_graficos = \"graficos_estacional_gap\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(carpeta_graficos, exist_ok=True)\n",
    "\n",
    "# === FUNCIÓN MAPE ===\n",
    "def calcular_mape(y_true, y_pred, min_denominador=1.0):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true >= min_denominador\n",
    "    return (\n",
    "        np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "        if np.any(mask)\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "# === PROPHET CON CDD/HDD, DUMMIES Y GAP VARIABLE ===\n",
    "def ejecutar_modelo_prophet_cdd_pmgd_proxy(\n",
    "    df,\n",
    "    columna,\n",
    "    fecha_inicio_test,\n",
    "    df_meteorologico,\n",
    "    nombre_estacion,\n",
    "    gap_weeks=0,\n",
    "    col_temp=\"Temp - °C\",\n",
    "    col_cdd=\"Días-grado de enfriamiento\",\n",
    "    col_hdd=\"Días-grado de calentamiento\",\n",
    "    mostrar_grafico=False,\n",
    "    ruta_guardado=None,\n",
    "):\n",
    "    \"\"\"Versión de Prophet con:\n",
    "    - CDD/HDD desde df_meteorologico.\n",
    "    - Proxy solar estacional ajustado por estación.\n",
    "    - Seasonality multiplicativa.\n",
    "    \"\"\"\n",
    "    # Preparar datos base\n",
    "    df_prophet = df[[\"Fecha y hora\", columna, \"estacion\"]].dropna().rename(columns={\n",
    "        \"Fecha y hora\": \"ds\", columna: \"y\"\n",
    "    })\n",
    "    df_prophet[\"ds\"] = pd.to_datetime(df_prophet[\"ds\"], errors=\"coerce\")\n",
    "    df_prophet = df_prophet.dropna(subset=[\"ds\"])\n",
    "    df_prophet = df_prophet[df_prophet[\"ds\"] <= pd.Timestamp(\"2025-12-31\")]\n",
    "\n",
    "    # Preparar meteo\n",
    "    meteo = df_meteorologico[df_meteorologico[\"Estacion\"] == nombre_estacion].copy()\n",
    "    meteo[\"Date & Time\"] = pd.to_datetime(meteo[\"Date & Time\"], errors=\"coerce\")\n",
    "    meteo = meteo.dropna(subset=[\"Date & Time\"])\n",
    "    meteo = meteo[meteo[\"Date & Time\"] <= pd.Timestamp(\"2025-12-31\")]\n",
    "\n",
    "    meteo = meteo[[\"Date & Time\", col_cdd, col_hdd]].rename(columns={\n",
    "        \"Date & Time\": \"ds\",\n",
    "        col_cdd: \"CDD\",\n",
    "        col_hdd: \"HDD\"\n",
    "    })\n",
    "    meteo[\"CDD2\"] = meteo[\"CDD\"] ** 2\n",
    "    meteo[\"HDD2\"] = meteo[\"HDD\"] ** 2\n",
    "\n",
    "    df_prophet = df_prophet.merge(meteo, on=\"ds\", how=\"left\")\n",
    "\n",
    "    # Proxy solar estacional\n",
    "    def _pmgd_prob(row):\n",
    "        hora_decimal = row[\"ds\"].hour + row[\"ds\"].minute / 60.0\n",
    "        est = row[\"estacion\"].lower()\n",
    "        if est == \"verano\":\n",
    "            return int(8 <= hora_decimal <= 19)\n",
    "        elif est == \"invierno\":\n",
    "            return int(10 <= hora_decimal <= 17)\n",
    "        else:\n",
    "            return int(9 <= hora_decimal <= 18)\n",
    "\n",
    "    df_prophet[\"pmgd_horario_probable\"] = df_prophet.apply(_pmgd_prob, axis=1)\n",
    "\n",
    "    # Dummies estación\n",
    "    dummies_estacion = pd.get_dummies(df_prophet[\"estacion\"], prefix=\"estacion\")\n",
    "    df_prophet = pd.concat([df_prophet, dummies_estacion], axis=1)\n",
    "\n",
    "    regresores = [\"CDD\", \"HDD\", \"CDD2\", \"HDD2\", \"pmgd_horario_probable\"] + dummies_estacion.columns.tolist()\n",
    "\n",
    "    # Fechas\n",
    "    fecha_inicio_test = pd.to_datetime(fecha_inicio_test)\n",
    "    fecha_fin_test = fecha_inicio_test + timedelta(days=7)\n",
    "    fecha_fin_train_eval = fecha_inicio_test - timedelta(weeks=gap_weeks, minutes=15)\n",
    "    fecha_inicio_train_eval = fecha_fin_train_eval - timedelta(weeks=1) + timedelta(minutes=15)\n",
    "\n",
    "    df_test = df_prophet[(df_prophet[\"ds\"] >= fecha_inicio_test) & (df_prophet[\"ds\"] < fecha_fin_test)].copy()\n",
    "    df_train_eval = df_prophet[(df_prophet[\"ds\"] >= fecha_inicio_train_eval) & (df_prophet[\"ds\"] < fecha_fin_train_eval)].copy()\n",
    "    df_train = df_prophet[df_prophet[\"ds\"] <= fecha_fin_train_eval].copy()\n",
    "\n",
    "    m = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        yearly_seasonality=False,\n",
    "        seasonality_mode=\"multiplicative\",\n",
    "        n_changepoints=80,\n",
    "        changepoint_prior_scale=0.1,\n",
    "        seasonality_prior_scale=10,\n",
    "    )\n",
    "    m.add_seasonality(name=\"daily_15m\", period=1, fourier_order=48)\n",
    "    m.add_seasonality(name=\"weekly_custom\", period=7, fourier_order=30)\n",
    "    m.add_seasonality(name=\"yearly_custom\", period=365.25, fourier_order=10)\n",
    "\n",
    "    for reg in regresores:\n",
    "        m.add_regressor(reg)\n",
    "\n",
    "    m.fit(df_train[[\"ds\", \"y\"] + regresores])\n",
    "\n",
    "    future = (\n",
    "        pd.concat([df_train_eval[[\"ds\"]], df_test[[\"ds\"]]])\n",
    "        .drop_duplicates()\n",
    "        .sort_values(\"ds\")\n",
    "        .merge(df_prophet[[\"ds\"] + regresores], on=\"ds\", how=\"left\")\n",
    "    )\n",
    "    forecast = m.predict(future)\n",
    "\n",
    "    forecast_train_eval = forecast[forecast[\"ds\"].isin(df_train_eval[\"ds\"])]\n",
    "    forecast_test = forecast[forecast[\"ds\"].isin(df_test[\"ds\"])]\n",
    "\n",
    "    df_train_eval = df_train_eval.set_index(\"ds\").join(\n",
    "        forecast_train_eval.set_index(\"ds\")[[\"yhat\"]], how=\"inner\"\n",
    "    )\n",
    "    df_test_eval = df_test.set_index(\"ds\").join(\n",
    "        forecast_test.set_index(\"ds\")[[\"yhat\"]], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    mae_train = mean_absolute_error(df_train_eval[\"y\"], df_train_eval[\"yhat\"])\n",
    "    rmse_train = np.sqrt(mean_squared_error(df_train_eval[\"y\"], df_train_eval[\"yhat\"]))\n",
    "    mape_train = calcular_mape(df_train_eval[\"y\"], df_train_eval[\"yhat\"])\n",
    "\n",
    "    mae_test = mean_absolute_error(df_test_eval[\"y\"], df_test_eval[\"yhat\"])\n",
    "    rmse_test = np.sqrt(mean_squared_error(df_test_eval[\"y\"], df_test_eval[\"yhat\"]))\n",
    "    mape_test = calcular_mape(df_test_eval[\"y\"], df_test_eval[\"yhat\"])\n",
    "\n",
    "    resultados_metricas = {\n",
    "        \"transformador\": columna,\n",
    "        \"fecha_inicio_test\": fecha_inicio_test.date(),\n",
    "        \"gap_semanas\": gap_weeks,\n",
    "        \"MAE_train\": mae_train,\n",
    "        \"RMSE_train\": rmse_train,\n",
    "        \"MAPE_train\": mape_train,\n",
    "        \"MAE_test\": mae_test,\n",
    "        \"RMSE_test\": rmse_test,\n",
    "        \"MAPE_test\": mape_test,\n",
    "    }\n",
    "\n",
    "    if mostrar_grafico or ruta_guardado:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df_test_eval.index, df_test_eval[\"y\"], label=\"Real\")\n",
    "        plt.plot(df_test_eval.index, df_test_eval[\"yhat\"], label=\"Predicción\")\n",
    "        plt.title(f\"{columna} [{fecha_inicio_test.date()}] – gap {gap_weeks} sem\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        if ruta_guardado:\n",
    "            plt.savefig(ruta_guardado)\n",
    "        if mostrar_grafico:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    df_test_eval = (\n",
    "        df_test_eval.reset_index()\n",
    "        .rename(columns={\"y\": \"real\", \"yhat\": \"predicho\"})\n",
    "        .assign(transformador=columna)\n",
    "    )\n",
    "\n",
    "    return resultados_metricas, df_test_eval\n",
    "\n",
    "# === PARAMETRIZACIÓN PARA LAS PRUEBAS DE STRESS ===\n",
    "inicio_data = pd.to_datetime(\"2023-06-24\")\n",
    "fin_data = pd.to_datetime(\"2025-04-02\")\n",
    "duracion_test = timedelta(days=7)\n",
    "\n",
    "estaciones_meses = {\n",
    "    \"primavera\": [9, 10, 11],\n",
    "    \"verano\": [12, 1, 2],\n",
    "    \"otoño\": [3, 4, 5],\n",
    "    \"invierno\": [6, 7, 8],\n",
    "}\n",
    "\n",
    "# Fechas disponibles filtradas\n",
    "fechas_disponibles = (\n",
    "    pd.Series(\n",
    "        pd.to_datetime(\n",
    "            df_transformadores[\"Fecha y hora\"].dropna().unique(),\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "    )\n",
    "    .dropna()\n",
    "    .sort_values()\n",
    ")\n",
    "\n",
    "\n",
    "def seleccionar_fechas_test(\n",
    "    meses_objetivo, gap_weeks, cantidad=2, min_dias_entre_fechas=21\n",
    "):\n",
    "    candidatos = []\n",
    "    fechas_filtradas = fechas_disponibles[\n",
    "        (fechas_disponibles.dt.weekday == 0)\n",
    "        & (fechas_disponibles.dt.hour == 0)\n",
    "        & (fechas_disponibles.dt.minute == 0)\n",
    "        & (fechas_disponibles.dt.month.isin(meses_objetivo))\n",
    "    ].sort_values(ascending=False)\n",
    "\n",
    "    for fecha in fechas_filtradas:\n",
    "        fecha_fin_train_eval = fecha - timedelta(weeks=gap_weeks)\n",
    "        if fecha_fin_train_eval <= inicio_data:\n",
    "            continue\n",
    "        if (fecha + duracion_test) > fin_data:\n",
    "            continue\n",
    "        if any(abs((fecha - f).days) < min_dias_entre_fechas for f in candidatos):\n",
    "            continue\n",
    "        candidatos.append(fecha)\n",
    "        if len(candidatos) == cantidad:\n",
    "            break\n",
    "    return candidatos\n",
    "\n",
    "print(\"Código cargado y listo.\")\n",
    "\n",
    "# === EJECUCIÓN DE LOS EXPERIMENTOS Y EXPORTACIÓN ===\n",
    "resultados_por_gap = {}\n",
    "predicciones_por_gap = {}\n",
    "\n",
    "for gap_weeks in [3, 6, 9, 12]:\n",
    "    resultados = []\n",
    "    predicciones_todas = []\n",
    "\n",
    "    for transformador in transformador_columnas_p:\n",
    "        for estacion, meses in estaciones_meses.items():\n",
    "            fechas = seleccionar_fechas_test(meses, gap_weeks)\n",
    "            for fecha in fechas:\n",
    "                try:\n",
    "                    nombre_archivo = (\n",
    "                        transformador.replace(\" \", \"_\")\n",
    "                        .replace(\".\", \"\")\n",
    "                        .replace(\"(\", \"\")\n",
    "                        .replace(\")\", \"\")\n",
    "                        .replace(\"/\", \"\")\n",
    "                    )\n",
    "                    fecha_str = fecha.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "                    ruta_grafico = os.path.join(\n",
    "                        carpeta_graficos,\n",
    "                        f\"{nombre_archivo}_{fecha_str}_gap{gap_weeks}.png\",\n",
    "                    )\n",
    "\n",
    "                    resultado, predicciones_test = ejecutar_modelo_prophet_cdd_pmgd_proxy(\n",
    "                        df=df_transformadores,\n",
    "                        columna=transformador,\n",
    "                        fecha_inicio_test=fecha,\n",
    "                        df_meteorologico=df_meteorologico,\n",
    "                        nombre_estacion=MAPEO_TRANSF_MEO[transformador],\n",
    "                        gap_weeks=gap_weeks,\n",
    "                        mostrar_grafico=False,\n",
    "                        ruta_guardado=ruta_grafico,\n",
    "                        )\n",
    "\n",
    "                    resultado[\"fecha_test\"] = fecha\n",
    "                    resultado[\"estacion\"] = estacion\n",
    "                    resultados.append(resultado)\n",
    "\n",
    "                    predicciones_test[\"fecha_test\"] = fecha\n",
    "                    predicciones_test[\"estacion\"] = estacion\n",
    "                    predicciones_test[\"gap_semanas\"] = gap_weeks\n",
    "                    predicciones_todas.append(predicciones_test)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\" Error en {transformador}, {fecha}, gap={gap_weeks}: {e}\")\n",
    "\n",
    "    resultados_por_gap[gap_weeks] = pd.DataFrame(resultados)\n",
    "    predicciones_por_gap[gap_weeks] = (\n",
    "        pd.concat(predicciones_todas, ignore_index=True) if predicciones_todas else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "# === GUARDADO EN EXCEL ===\n",
    "nombre_excel = os.path.join(output_dir, \"resultados_prophet_estacional_por_gap_P_invierten_mejorado_2.xlsx\")\n",
    "with pd.ExcelWriter(nombre_excel, engine=\"openpyxl\") as writer:\n",
    "    for gap_weeks in resultados_por_gap:\n",
    "        hoja_metricas = f\"Metricas_gap{gap_weeks}\"\n",
    "        hoja_preds = f\"Predicciones_gap{gap_weeks}\"\n",
    "        resultados_por_gap[gap_weeks].to_excel(writer, sheet_name=hoja_metricas, index=False)\n",
    "        predicciones_por_gap[gap_weeks].to_excel(writer, sheet_name=hoja_preds, index=False)\n",
    "\n",
    "print(\" Resultados guardados correctamente con estacionalidades y gap (sin límite de 1 año).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e682b-2c97-4b33-b58f-dba0b119fe37",
   "metadata": {},
   "source": [
    "## Test festivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e44343b6-6040-4c46-8b02-80293a8db128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_modelo_prophet_cdd_pmgd_proxy(\n",
    "    df,\n",
    "    columna,\n",
    "    fecha_inicio_test,\n",
    "    df_meteorologico,\n",
    "    nombre_estacion,\n",
    "    col_temp=\"Temp - °C\",\n",
    "    col_cdd=\"Días-grado de enfriamiento\",\n",
    "    col_hdd=\"Días-grado de calentamiento\",\n",
    "    mostrar_grafico=False,\n",
    "    ruta_guardado=None,\n",
    "):\n",
    "    \"\"\"Versión de Prophet con:\n",
    "    - Lectura de CDD/HDD desde df_meteorologico.\n",
    "    - Proxy solar estacional ajustado por estación del año en Chile.\n",
    "    - Seasonality multiplicativa y Fourier daily/weekly/yearly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preparar datos base\n",
    "    df_prophet = df[[\"Fecha y hora\", columna, \"estacion\"]].dropna().rename(columns={\n",
    "        \"Fecha y hora\": \"ds\", columna: \"y\"\n",
    "    })\n",
    "    df_prophet[\"ds\"] = pd.to_datetime(df_prophet[\"ds\"])\n",
    "\n",
    "    # Preparar meteo (con CDD y HDD incluidos)\n",
    "    meteo = df_meteorologico[df_meteorologico[\"Estacion\"] == nombre_estacion].copy()\n",
    "    meteo[\"Date & Time\"] = pd.to_datetime(meteo[\"Date & Time\"])\n",
    "    meteo = meteo[[\"Date & Time\", col_cdd, col_hdd]].rename(columns={\n",
    "        \"Date & Time\": \"ds\",\n",
    "        col_cdd: \"CDD\",\n",
    "        col_hdd: \"HDD\"\n",
    "    })\n",
    "    meteo[\"CDD2\"] = meteo[\"CDD\"] ** 2\n",
    "    meteo[\"HDD2\"] = meteo[\"HDD\"] ** 2\n",
    "\n",
    "    # Merge a df_prophet\n",
    "    df_prophet = df_prophet.merge(meteo, on=\"ds\", how=\"left\")\n",
    "\n",
    "    # Proxy solar estacional\n",
    "    def _pmgd_prob(row):\n",
    "        hora_decimal = row[\"ds\"].hour + row[\"ds\"].minute / 60.0\n",
    "        est = row[\"estacion\"].lower()\n",
    "        if est == \"verano\":\n",
    "            return int(8 <= hora_decimal <= 19)\n",
    "        elif est == \"invierno\":\n",
    "            return int(10 <= hora_decimal <= 17)\n",
    "        else:\n",
    "            return int(9 <= hora_decimal <= 18)\n",
    "\n",
    "    df_prophet[\"pmgd_horario_probable\"] = df_prophet.apply(_pmgd_prob, axis=1)\n",
    "\n",
    "    # Dummies estación\n",
    "    dummies_estacion = pd.get_dummies(df_prophet[\"estacion\"], prefix=\"estacion\")\n",
    "    df_prophet = pd.concat([df_prophet, dummies_estacion], axis=1)\n",
    "\n",
    "    regresores = [\"CDD\", \"HDD\", \"CDD2\", \"HDD2\", \"pmgd_horario_probable\"] + dummies_estacion.columns.tolist()\n",
    "\n",
    "    # Fechas\n",
    "    fecha_inicio_test = pd.to_datetime(fecha_inicio_test)\n",
    "    fecha_fin_test = fecha_inicio_test + pd.Timedelta(days=1)\n",
    "\n",
    "    fecha_fin_train_eval = fecha_inicio_test - pd.Timedelta(weeks=3)\n",
    "    fecha_inicio_train_eval = fecha_fin_train_eval - pd.Timedelta(days=1)\n",
    "    \n",
    "    df_test = df_prophet[(df_prophet[\"ds\"] >= fecha_inicio_test) & (df_prophet[\"ds\"] < fecha_fin_test)].copy()\n",
    "    df_train_eval = df_prophet[(df_prophet[\"ds\"] >= fecha_inicio_train_eval) & (df_prophet[\"ds\"] < fecha_fin_train_eval)].copy()\n",
    "    df_train = df_prophet[df_prophet[\"ds\"] <= fecha_fin_train_eval].copy()\n",
    "\n",
    "    # Prophet\n",
    "    m = Prophet(\n",
    "        daily_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        yearly_seasonality=False,\n",
    "        seasonality_mode=\"multiplicative\",\n",
    "        n_changepoints=80,\n",
    "        changepoint_prior_scale=0.1,\n",
    "        seasonality_prior_scale=10,\n",
    "    )\n",
    "\n",
    "    m.add_seasonality(name=\"daily_15m\", period=1, fourier_order=48)\n",
    "    m.add_seasonality(name=\"weekly_custom\", period=7, fourier_order=30)\n",
    "    m.add_seasonality(name=\"yearly_custom\", period=365.25, fourier_order=10)\n",
    "\n",
    "    for reg in regresores:\n",
    "        m.add_regressor(reg)\n",
    "\n",
    "    m.fit(df_train[[\"ds\", \"y\"] + regresores])\n",
    "\n",
    "    future = pd.concat([df_train_eval[[\"ds\"]], df_test[[\"ds\"]]]).drop_duplicates().sort_values(\"ds\")\n",
    "    future = future.merge(df_prophet[[\"ds\"] + regresores], on=\"ds\", how=\"left\")\n",
    "\n",
    "    forecast = m.predict(future)\n",
    "    forecast_train_eval = forecast[forecast[\"ds\"].isin(df_train_eval[\"ds\"])]\n",
    "    forecast_test = forecast[forecast[\"ds\"].isin(df_test[\"ds\"])]\n",
    "\n",
    "    df_train_eval = df_train_eval.set_index(\"ds\").join(\n",
    "        forecast_train_eval.set_index(\"ds\")[[\"yhat\"]], how=\"inner\"\n",
    "    )\n",
    "    df_test_eval = df_test.set_index(\"ds\").join(\n",
    "        forecast_test.set_index(\"ds\")[[\"yhat\"]], how=\"inner\"\n",
    "    )\n",
    "\n",
    "    mae_train = mean_absolute_error(df_train_eval[\"y\"], df_train_eval[\"yhat\"])\n",
    "    rmse_train = np.sqrt(mean_squared_error(df_train_eval[\"y\"], df_train_eval[\"yhat\"]))\n",
    "    mape_train = calcular_mape(df_train_eval[\"y\"], df_train_eval[\"yhat\"])\n",
    "\n",
    "    mae_test = mean_absolute_error(df_test_eval[\"y\"], df_test_eval[\"yhat\"])\n",
    "    rmse_test = np.sqrt(mean_squared_error(df_test_eval[\"y\"], df_test_eval[\"yhat\"]))\n",
    "    mape_test = calcular_mape(df_test_eval[\"y\"], df_test_eval[\"yhat\"])\n",
    "\n",
    "    resultados_metricas = {\n",
    "        \"transformador\": columna,\n",
    "        \"fecha_inicio_test\": fecha_inicio_test.date(),\n",
    "        \"MAE_train\": mae_train,\n",
    "        \"RMSE_train\": rmse_train,\n",
    "        \"MAPE_train\": mape_train,\n",
    "        \"MAE_test\": mae_test,\n",
    "        \"RMSE_test\": rmse_test,\n",
    "        \"MAPE_test\": mape_test,\n",
    "    }\n",
    "\n",
    "    if mostrar_grafico or ruta_guardado:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df_test_eval.index, df_test_eval[\"y\"], label=\"Real\")\n",
    "        plt.plot(df_test_eval.index, df_test_eval[\"yhat\"], label=\"Predicción\")\n",
    "        plt.title(f\"{columna} [{fecha_inicio_test.date()}]\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if ruta_guardado:\n",
    "            plt.savefig(ruta_guardado)\n",
    "        if mostrar_grafico:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    df_test_eval = df_test_eval.reset_index()\n",
    "    df_test_eval[\"transformador\"] = columna\n",
    "    df_test_eval = df_test_eval.rename(columns={\"y\": \"real\", \"yhat\": \"predicho\"})\n",
    "\n",
    "    return resultados_metricas, df_test_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "653f6bab-14e0-4762-9760-0b146f00589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Años considerados para feriados: [2024, 2025]\n",
      "Rango de fechas: 2024-04-01 hasta 2025-03-30\n",
      "Feriados en rango filtrados:\n",
      "2024-05-01 00:00:00\n",
      "2024-05-21 00:00:00\n",
      "2024-06-20 00:00:00\n",
      "2024-06-29 00:00:00\n",
      "2024-07-16 00:00:00\n",
      "2024-08-15 00:00:00\n",
      "2024-09-18 00:00:00\n",
      "2024-09-19 00:00:00\n",
      "2024-09-20 00:00:00\n",
      "2024-10-12 00:00:00\n",
      "2024-10-31 00:00:00\n",
      "2024-11-01 00:00:00\n",
      "2024-12-08 00:00:00\n",
      "2024-12-25 00:00:00\n",
      "2025-01-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:28:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:29:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:29:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:30:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:30:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:30:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:31:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:31:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:31:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:32:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:32:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:33:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:33:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:35:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:35:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:36:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:36:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:37:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:37:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:38:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:38:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:39:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:39:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:40:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:40:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:42:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:42:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:42:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:42:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:43:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:44:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:44:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:44:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:45:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:45:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:46:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:46:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:47:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:47:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:47:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:50:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:50:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:51:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:51:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:52:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:52:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:53:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:54:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:54:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:55:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:55:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:56:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:56:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:57:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:57:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:58:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:58:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:59:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:59:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:00:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:00:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:00:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:00:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:01:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:02:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:03:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:03:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:04:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:04:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:05:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:05:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:06:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:06:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:07:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:07:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:08:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:08:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:09:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:09:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:11:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:11:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:12:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:12:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:13:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:13:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:14:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:14:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:15:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:15:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:15:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:15:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:16:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:16:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:17:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:17:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:17:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:17:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:18:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:18:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:20:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:20:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:21:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:21:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:23:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:23:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:24:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:24:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:26:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:26:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:27:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:27:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:30:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:30:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:31:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:31:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:33:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:33:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:34:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:34:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:36:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:36:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:37:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:37:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:39:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:39:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:41:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:41:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:43:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:44:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:46:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:46:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:48:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:48:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:51:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:51:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:53:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:53:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:55:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:55:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:57:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:57:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:59:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "12:59:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:01:30 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       transformador fecha_inicio_test  \\\n",
      "0  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-05-01   \n",
      "1  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-05-21   \n",
      "2  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-06-20   \n",
      "3  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-06-29   \n",
      "4  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ...        2024-07-16   \n",
      "\n",
      "   MAE_train  RMSE_train  MAPE_train   MAE_test  RMSE_test   MAPE_test  \\\n",
      "0   2.188176    3.125746   30.650954  51.094230  51.153002  941.466555   \n",
      "1   2.681245    3.425549   14.013599  16.364049  16.619316  185.020971   \n",
      "2   2.719620    3.422708   31.587197   3.599468   4.330810   28.610587   \n",
      "3   1.596128    1.905423   23.561387   1.867803   2.163077   20.438414   \n",
      "4   5.244825    6.396396   32.531429   3.055378   3.821947   17.675025   \n",
      "\n",
      "  fecha_test                                festivo  anio  \n",
      "0 2024-05-01               Día Nacional del Trabajo  2024  \n",
      "1 2024-05-21             Día de las Glorias Navales  2024  \n",
      "2 2024-06-20  Día Nacional de los Pueblos Indígenas  2024  \n",
      "3 2024-06-29                  San Pedro y San Pablo  2024  \n",
      "4 2024-07-16                      Virgen del Carmen  2024  \n",
      "                   ds       real estacion  CDD    HDD  CDD2      HDD2  \\\n",
      "0 2024-05-01 00:00:00  12.122226    otoño  0.0  0.069   0.0  0.004761   \n",
      "1 2024-05-01 00:15:00  11.676122    otoño  0.0  0.070   0.0  0.004900   \n",
      "2 2024-05-01 00:30:00  11.249320    otoño  0.0  0.071   0.0  0.005041   \n",
      "3 2024-05-01 00:45:00  11.068930    otoño  0.0  0.072   0.0  0.005184   \n",
      "4 2024-05-01 01:00:00  10.812145    otoño  0.0  0.073   0.0  0.005329   \n",
      "\n",
      "   pmgd_horario_probable  estacion_invierno  estacion_otoño  \\\n",
      "0                      0              False            True   \n",
      "1                      0              False            True   \n",
      "2                      0              False            True   \n",
      "3                      0              False            True   \n",
      "4                      0              False            True   \n",
      "\n",
      "   estacion_primavera  estacion_verano   predicho  \\\n",
      "0               False            False -38.370557   \n",
      "1               False            False -37.966420   \n",
      "2               False            False -39.199605   \n",
      "3               False            False -38.827765   \n",
      "4               False            False -39.957135   \n",
      "\n",
      "                                       transformador fecha_test  \\\n",
      "0  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-05-01   \n",
      "1  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-05-01   \n",
      "2  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-05-01   \n",
      "3  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-05-01   \n",
      "4  SE_San_Rafael.Trf_San_Rafael_T1 Potencia activ... 2024-05-01   \n",
      "\n",
      "                    festivo  anio  \n",
      "0  Día Nacional del Trabajo  2024  \n",
      "1  Día Nacional del Trabajo  2024  \n",
      "2  Día Nacional del Trabajo  2024  \n",
      "3  Día Nacional del Trabajo  2024  \n",
      "4  Día Nacional del Trabajo  2024  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import holidays\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1)  Carpeta donde guardar los PNG\n",
    "# -------------------------------------------------------------------\n",
    "carpeta_graficos = Path(\"graficos_resultados_festivos_CL\")\n",
    "carpeta_graficos.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Rango de fechas deseado\n",
    "# -------------------------------------------------------------------\n",
    "fecha_inicio = pd.Timestamp(\"2024-04-01\")\n",
    "fecha_fin    = pd.Timestamp(\"2025-03-30\")\n",
    "\n",
    "years = list(range(fecha_inicio.year, fecha_fin.year + 1))\n",
    "\n",
    "print(f\"Años considerados para feriados: {years}\")\n",
    "print(f\"Rango de fechas: {fecha_inicio.date()} hasta {fecha_fin.date()}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Construir feriados para esos años y filtrar rango\n",
    "# -------------------------------------------------------------------\n",
    "feriados_cl = holidays.CountryHoliday(\"CL\", years=years)\n",
    "\n",
    "fechas_en_rango = [\n",
    "    pd.Timestamp(d)\n",
    "    for d in feriados_cl\n",
    "    if fecha_inicio <= pd.Timestamp(d) <= fecha_fin\n",
    "]\n",
    "\n",
    "print(\"Feriados en rango filtrados:\")\n",
    "for f in fechas_en_rango:\n",
    "    print(f)\n",
    "\n",
    "fechas_test_por_festivo = {}\n",
    "for fecha in fechas_en_rango:\n",
    "    anio = fecha.year\n",
    "    fechas_test_por_festivo.setdefault(anio, []).append(fecha)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4)  Bucle principal – sin cambios …\n",
    "# -------------------------------------------------------------------\n",
    "resultados = []\n",
    "predicciones_todas = []\n",
    "\n",
    "for transformador in transformador_columnas_p:\n",
    "    for anio, fechas in fechas_test_por_festivo.items():\n",
    "        for fecha in fechas:\n",
    "            # --- verificación rápida de datos válidos (opcional) ---\n",
    "            sub_df = df_transformadores[df_transformadores['Fecha y hora'].dt.date == fecha.date()]\n",
    "            if sub_df[transformador].dropna().shape[0] < 2:\n",
    "                print(f\"Saltando {transformador}, festivo {fecha.date()}: <2 datos válidos\")\n",
    "                continue\n",
    "            # --------------------------------------------------------\n",
    "\n",
    "            try:\n",
    "                fecha_str = fecha.strftime(\"%Y-%m-%d\")\n",
    "                nombre_archivo = (\n",
    "                    transformador.replace(\" \", \"_\")\n",
    "                                 .replace(\".\", \"_\")\n",
    "                                 .replace(\"(\", \"\")\n",
    "                                 .replace(\")\", \"\")\n",
    "                                 .replace(\"/\", \"_\")\n",
    "                )\n",
    "                ruta_grafico = carpeta_graficos / f\"{nombre_archivo}_{fecha_str}.png\"\n",
    "\n",
    "                resultado, predicciones_test = ejecutar_modelo_prophet_cdd_pmgd_proxy(\n",
    "                    df_transformadores,\n",
    "                    columna=transformador,\n",
    "                    fecha_inicio_test=fecha,\n",
    "                    df_meteorologico=df_meteorologico,\n",
    "                    nombre_estacion=MAPEO_TRANSF_MEO[transformador],\n",
    "                    mostrar_grafico=False,\n",
    "                    ruta_guardado=ruta_grafico\n",
    "                )\n",
    "\n",
    "                resultado.update({\n",
    "                    \"transformador\": transformador,\n",
    "                    \"fecha_test\":    fecha,\n",
    "                    \"festivo\":       feriados_cl.get(fecha),\n",
    "                    \"anio\":          anio\n",
    "                })\n",
    "                resultados.append(resultado)\n",
    "\n",
    "                predicciones_test[\"fecha_test\"] = fecha\n",
    "                predicciones_test[\"festivo\"]    = feriados_cl.get(fecha)\n",
    "                predicciones_test[\"anio\"]       = anio\n",
    "                predicciones_todas.append(predicciones_test)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en {transformador}, festivo {fecha}: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5)  Consolidar resultados\n",
    "# -------------------------------------------------------------------\n",
    "df_resultados   = pd.DataFrame(resultados)\n",
    "df_predicciones = pd.concat(predicciones_todas, ignore_index=True)\n",
    "\n",
    "print(df_resultados.head())\n",
    "print(df_predicciones.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5106c5be-b750-40b7-af30-025697c0bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"resultados_prophet_P_no_invierte_festivos.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_resultados.to_excel(writer, sheet_name=\"Metricas\", index=False)\n",
    "    df_predicciones.to_excel(writer, sheet_name=\"Predicciones\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf76d7a-9a27-41d2-a997-6723fa7fb8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
